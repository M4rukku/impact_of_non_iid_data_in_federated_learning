# Part II Project - The Impact of Non-IID Data on Federated Learning
This is the Github Repository for my Cambridge Part II project on the impact of
 non-iid data on federated learning.
 
The following Readme will contain notes on setup, information about the
 experiments and acknowledgements for reused code. The project is supposed to
  be executed on a unix based operating system since it uses shell scripts
   in some parts (for setting up the data pipelines). There is
    a workaround for Windows users which is described in Section 
    [Execute the Dataloader Script on Windows](#windows_script_fix).
 
## Setup

### Package Management
To initialise the project, first initialse a new virtual environment. The git
repository contains a requirements.txt file which contains all necessary
 packages. The minimum version of Python required is **Python 3.8**.

```
$ virtualenv <env_name> 
$ source <env_name>/bin/activate (<env_name>)
$ pip install -r requirements.txt 
```

### Dataset Setup
Then fetch and preprocess the datasets you are interested into with the
 following script:
```
$ python initialise_datasets.py [--celeba] [--femnist] [--shakespeare] 
```
This might take a lot of memory (up to 35GB), so ensure that you initialise this
repository on a drive with enough storage. You will spend several hours when unzipping the 
Celeba files, so keep that in mind when installing them. I 
recommend initialising each
 dataset individually and in parallel (open a terminal for each). This will make
  the process finish far quicker since we can make use of multiple processors.

#### Setting up IID Datasets
For centralised evaluation and baseline (IID) convergence models, we utilise IID datasets. These
 datasets are generated by the create_iid_datasets.py script.
 
 Please execute this script:
 ```
python create_iid_datasets.py --all
```

The first line will create the complete datasets as pickle files. The other, will create
 fractional datasets which are more lightweight for use in evaluation.
 
#### Setting up IID Client Datasets
For the baseline comparison (impact of non-iidness of data), we use a set of IID clients. 
These may be created with the following script. Note that you have to create the full iid
 datasets first (otherwise the script will fail).
```
python create_iid_clients.py --all
```

#### Setting up the Celeba Lda Datasets

The script create_lda_cifar_datasets creates custom non-iid datasets based on sampling the 
Cifar10 dataset with a latent dirichlet distribution. 
(see [here](https://arxiv.org/pdf/1909.06335.pdf))

By pasting the --num_partitions and --concentration parameters to the script, you can create 
custom clients for your experiments. By default, it will set num_partitions to 100 (500 images 
per client) and the concentrations to the list [0.001, 0.5, 100.0], as described in the 
above paper. To start the script you always need to pass --start.

```
python create_lda_cifar_datasets.py --start 
```

or for optional settings:
```
python create_lda_cifar_datasets.py --start  --num_partitions 200 --concentrations 10 111 2
```

### GPU Acceleration

This project uses Tensorflow 2.x to run the code. Therefore, ensure that you
 have installed the latest version of CUDA and the necessary NVIDIA drivers
  on your system. (https://www.tensorflow.org/install/gpu)

### Recommended: Enable Long Paths on Windows
The simulator will create very long paths during experimentation. Either be aware and choose
 short experiment names, or enable long paths globally. You can find the instructions for that
  [here](https://docs.microsoft.com/en-us/windows/win32/fileio/maximum-file-path-limitation?tabs=cmd)

In short, you need to execute the following script in a privileged powershell prompt on Windows
 10, 1607 or later and then restart the computer.
 
 ```
New-ItemProperty -Path "HKLM:\SYSTEM\CurrentControlSet\Control\FileSystem" `
-Name "LongPathsEnabled" -Value 1 -PropertyType DWORD -Force
```
### Execute the Dataloader Script on Windows
 
<a id="windows_script_fix">
   
</a>

The initialise_datasets.py script may fail on windows since it tries to
execute shell scripts as subprocesses. To circumvent this issue please
install e.g. Ubuntu on the WSL. You can find the instructions 
[here](https://ubuntu.com/wsl). 
   
First, ensure that you have both python3 and pip3 installed on the
 WSL. (On Ubuntu 20.04+ python3 is preinstalled. You can check if python3/pip3
  is preinstalled by typing `python3 -v`.)
 
Should you need to install python beforehand; use the deadsnakes ppa: 

```
$ sudo apt-get install software-properties-common
$ sudo add-apt-repository ppa:deadsnakes/ppa
$ sudo apt-get update
$ sudo apt-get install python3.8
```

Check that the installation succeeded by executing `python3 -v` and `pip3 -v`. In general, I
 recommend to at least execute `sudo apt update` and `sudo apt upgrade`before proceeding.

If you don't have pip3 installed, please execute the following.

```
$ sudo apt update
$ sudo apt install python3-pip
```

Now install virtualenv on the environment:

```
$ pip3 install virtualenv
or
$ python3 -m pip install virtualenv
```

Note that the WSL allows accessing the Windows filesystem on /mnt/path/to
/project. Now, navigate to the project root folder on windows (by going
 through /mnt/drive...). We need to create a new virtual environment on linux 
 to load all necessary packages to run the dataset loading script.

```
$ python3 -m venv linux_venv        #Create virtual environment
$ source ./linux_venv/bin/activate  #Activate new Virtual Environment
$ pip install -r requirements.txt   #Install all necessary packages
```

Finally, we need to change the line endings from CRLF (Windows) to LF
 (Linux), so that the scripts will execute correctly. For this we will use a
  tool called dos2unix.

```
$ sudo apt install dos2unix
```

Then we need to change all python and shell scripts within leaf_root and
 data_loader folders to use LF.
 
```
$ find leaf_root data_loaders -maxdepth 4 -type f -name "*.sh" | xargs dos2unix
$ find leaf_root data_loaders -maxdepth 4 -type f -name "*.py" | xargs dos2unix
```

or in a more succinct manner:

```
$ find leaf_root data_loaders -maxdepth 4 -type f -regex ".*/.*\.\(\(py\)\|\(sh\)\)" | xargs dos2unix
```

Now you can finally run the dataloading script:

```
python initialise_datasets.py [--celeba] [--shakespeare] [--femnist]
```

You might want to consider reverting the change in line endings after
 completion. For this use the tool unix2dos similarly (this tool will get
  installed together with dos2unix). 
 
```
$ find leaf_root data_loaders -maxdepth 4 -type f -name "*.sh" | xargs unix2dos
$ find leaf_root data_loaders -maxdepth 4 -type f -name "*.py" | xargs unix2dos
```

or in a more succinct manner: 

```
$ find leaf_root data_loaders -maxdepth 4 -type f -regex ".*/.*\.\(\(py\)\|\(sh\)\)" | xargs unix2dos
```

## Experiments

### Notes on Execution
Depending on the use-case and simulator, the program will take a lot of System Resources. Ensure
 that you have enough RAM, Processors and Virtual Memory allocated on your PC. Note that
  simulations executed with Ray will be vastly more efficient than those executed with
   Multiprocessing. If you see your system struggling with the load, try to increase those
    parameters.

### Notes on Execution (Windows)

Note that executing an experiment might take a lot of system resource. In
 Windows, in particular I had issues with large number of clients crashing
  on load-up. To allow a process to start with more resources, please make
   use of the Windows [`start`](https://docs.microsoft.com/en-us/windows-server/administration/windows-commands/start) 
   command.
   
First, activate the virtual environment we set up before.
Then, move towards the root of the project and execute i.e. 

```
start "" /high /wait /b python "path/to/experiment.py"
```

or in Powershell

```
($Process = Start-Process -FilePath "python" -WorkingDirectory "." -Wait -ArgumentList
 ".\experiments\initial_experiments\femnist_full_experiment.py" -NoNewWindow).PriorityClass
 = [System.Diagnostics.ProcessPriorityClass]::High
```
## Acknowledgements

All code which I have copied fully or partially is acknowledged below
. All license statements have been copied and compiled into the LICENSES File
 (in the base folder).

In particular I would like to acknowledge the following two projects from
 which I have reused code in my project: 
 - LEAF (https://leaf.cmu.edu/) - BSD License 2.0
 - Flower (https://flower.dev/) - Apache License 2.0
 
 ### (Partial) Code Reuse
 
 - The leaf_root subfolder contains a clone of the LEAF Github repository
   (https://github.com/TalwalkarLab/leaf), which I use to download and
    preprocess data according to their [paper](https://arxiv.org/abs/1812.01097). 
    (BSD 2 License)
  
 - The flower github contains some scripts for preprocessing the data from
  LEAF. I could completely reuse two of those 
  [scripts](https://github.com/adap/flower/tree/ada3e12622187ae6f4b1f23aef576e23faa19674/baselines/flwr_baselines/scripts/leaf) (Shakespeare, Femnist
  ) and I adapted them for use with Celeba. (https://github.com/adap/flower)
  
 - Since I am using the Flower Framework for Federated Learning, I have also
  taken inspiration from their examples. In particular, I have tried looked
   at the way to use Tensorflow in their [advanced_tensorflow script](https://github.com/adap/flower/tree/f772993df9212b3e96b8fa916fdc1ecbe96500c2/examples/advanced_tensorflow)
   and the way the team creates [simulations](https://github.com/adap/flower/tree/f772993df9212b3e96b8fa916fdc1ecbe96500c2/examples/simulation) with flwr.
 